---
title: "Code Performance for larger simulations"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Code Performance for larger simulations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
This vignette studies the performance of the dynamic TOPMODEL code for 'larger'
simulations. The scale of a dynamic TOPMODEL simualtions is related to a
number of factors given below along with the upper limits considered in the discussion

```{r, echo=FALSE, results='asis'}
factors <- data.frame(Factor = c("The number of time steps to be evaluated",
                                 "The number of input series",
                                 "The number of hillslope HRU",
                                 "The number of channel HRU",
                                 "The number of computational steps"),
                      Value = c(1000000,1000,5000,1000,3000000),
                      Comment = c("Approximatly 30 years of 15 minute data",
                                  "Enough to allow for some representation of spatial input",
                                  "Approximatly 10 hillsope units for each spatial input",
                                  "Enough to output 1000km of river in 1km lengths for a hydraulic model",
                                  "Approximatly running 30 years of 15 minute data with 3 sub steps (5 minute step)"),
                      stringsAsFactors=FALSE)
knitr::kable(factors,caption="Factor affecting performance and limits considered")
```

In the following section scaling up in terms of size of the input and output
data is discussed. The performance of simulations with increasing numbers of
hillslope HRUs is then tested and the code profiled to suggest improvements.

# Scaling the input and output data

The size of the input data relates to the product of the number of timesteps
and input series. Similarly the size of the output series relates to the
product of the number of timesteps and number of channel HRUs.

The \code{dynatop} function holds all its values within memory (including swap if
enabled). The amount of memory used for the storage of variables can be broken into different classes for
which approximate values for the limits outlined are shown below. 

```{r, echo=FALSE, results='asis'}
mem_usage<- data.frame(Usage= c("Storage of input data",
                                "Storage of output data",
                                "Storage of hillslope states & parameters",
                                "Storage of channel states"),
                       Value = c(factors[1,'Value']*factors[2,'Value'],
                                factors[1,'Value']*factors[4,'Value'] + factors[3,'Value']*20,
                                factors[3,'Value']*20,
                                factors[4,'Value']*4),
                       stringsAsFactors=FALSE)
mem_usage[,'Value'] <- signif(mem_usage[,'Value']*8/1e6,1)
knitr::kable(mem_usage,caption="Approximate memory usage in MB assuming a 8 bit numeric values")
```

R has at times quite complex memory usage patterns (see
[here](http://adv-r.had.co.nz/memory.html) for a fuller introduction) but the
above table shows that it is likely that memory usage is going to dominate the
scaling performance as the input and output data size. increases.

The simpliest method to address memory usage limits is to split the input
 data into sequential chunks (e.g. a year at a time) then evaluate them
 initialising the model with the final states from the previous chunk. Then by
 reading the input and writing the output for each chunk individually the
 memory use can be constrained. The \code{dynatop}
function allows for this, returning the final state and allowing initial
states to be provided. It is left to the user to script the remainder based on
their specific requirements.

# Scaling with increasing numbers of hillslope HRUs and computational steps
The computational work load of executing \code{dynatop} is dominated by the
evaluation of the evolution of the hillslope HRUs. For each computational step
the equation given in the [accompanying
vignette](https://waternumbers.github.io/dynatop/articles/The_solution_of_Dynamic_TOPMODEL.html)
are solved. The execution time of the code therefor will depend on both the
number of hillslope HRUs and number of computational steps.

To test this simulations are performed based on the included Brompton
data. Each simulation last for 365*4 timesteps. Additional hillslope HRU are
generated by replicating the existing hillslope HRUs and associated routing
matrices. The number of channels is kept constant.

The following code is used to generate the results.
```{r, perf_eval}
library(dynatop)
data('brompton')

## tidy up the model so it runs
model <- brompton$model
current_total_frac_sat <- colSums( rbind(model$Wsat,model$Fsat) )
current_total_frac_ex <- colSums( rbind(model$Wex,model$Fex) )
model$Wsat[,'101'] <- model$Wsat[,'101']/current_total_frac_sat['101']
model$Fsat[,'101'] <- model$Fsat[,'101']/current_total_frac_sat['101']
model$Wex[,'101'] <- model$Wex[,'101']/current_total_frac_ex['101']
model$Fex[,'101'] <- model$Fex[,'101']/current_total_frac_ex['101']

## create input data
rain <- resample_xts(brompton$rain, dt = 15/60)
obs <- merge(rain,brompton$pet,all=c(TRUE,FALSE))
obs <- merge(obs,brompton$qobs,all=c(TRUE,FALSE))
length(obs)

fperf <- function(model,obs,nhru,nrep,nstep){
    hill <- do.call(rbind, replicate(nhru, model$hillslope, simplify=FALSE))
    hill$id <- 1:nrow(hill)
    ## construct matrices
    W <- as.matrix(Matrix::bdiag(rep(list(model$Wex),nhru)))
    F <- do.call(cbind, replicate(nhru, model$Fex, simplify=FALSE))
    rownames(W) <- colnames(W) <- colnames(F) <- paste(hill$id)

    model$hillslope <- hill
    model$Wex <- model$Wsat <- W
    model$Fex <- model$Fsat <- F

    gc()
    replicate(nrep,system.time(dynatop(model,obs,nstep)))
}

test_hru <- c(1,seq(100,500,by=100))
test_step <- 1:5
samp <- expand.grid(test_hru,test_step)
out <- list()
for(ii in 1:nrow(samp)){
    out[[ii]] <- fperf(model,obs_data,samp[ii,1],10,samp[ii,2])
}
```


# Code Profiling



# Appendix
The simulations were performed on a machine with equiped with Intel Core i7
CPU 920 @2.67 Ghz processors and 11.7 Gb of RAM. The session information is:
```{r session_info}
sessionInfo()
```
